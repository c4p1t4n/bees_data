version: "3.8"

services:
  airflow:
    build:
      context: .
      dockerfile: Dockerfile
    image: airflow_custom
    container_name: airflow
    ports:
      - "8080:8080"
    networks:
      - airflow-net
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__WEBSERVER__SECRET_KEY: my_secret
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AWS_ACCESS_KEY_ID_EMAIL: configurar
      AWS_SECRET_ACCESS_KEY_EMAIL: configurar
      AWS_ACCESS_KEY_ID: airflowuser
      AWS_SECRET_ACCESS_KEY: airflowpass123
      S3_ENDPOINT_URL: http://minio:9000
      BRONZE_BUCKET: datalake
      PYTHONPATH: /opt/airflow:/opt/airflow/dags
    volumes:
      - ./src:/opt/airflow/src
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
    entrypoint: /bin/bash
    command: >
      -c "
      pip install apache-airflow-providers-docker;
      airflow db init &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com;
      airflow scheduler &
      exec airflow webserver
      "

  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      - airflow-net
    volumes:
      - minio_data:/data
    environment:
      MINIO_ROOT_USER: airflowuser
      MINIO_ROOT_PASSWORD: airflowpass123
    command: server --console-address ":9001" /data

  createbucket:
    image: minio/mc
    depends_on:
      - minio
    networks:
      - airflow-net
    entrypoint: >
      /bin/sh -c "
      sleep 5;
      mc alias set local http://minio:9000 airflowuser airflowpass123;
      mc mb -p local/datalake;
      mc policy set public local/datalake;
      "

networks:
  airflow-net:

volumes:
  minio_data:
